{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, BertConfig,\n",
    "   )\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "rmse = evaluate.load('mse')\n",
    "\n",
    "# define an evaluation function to pass into trainer later\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply untrained model on text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders.task1 import dataLoader as dataLoader1\n",
    "from dataloaders.task2 import dataLoader as dataLoader2\n",
    "from dataloaders.task3 import dataLoader as dataLoader3\n",
    "from dataloaders.task4 import dataLoader as dataLoader4\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# task1_train_dataloader,task1_val_dataloader,task1_test_dataloader = dataLoader1(base_model_name,batch_size=batch_size)\n",
    "# task2_train_dataloader,task2_val_dataloader,task2_test_dataloader = dataLoader2(base_model_name,batch_size=batch_size)\n",
    "# task3_train_dataloader,task3_val_dataloader = dataLoader3(base_model_name,batch_size=batch_size)\n",
    "# task4_train_dataloader,task4_val_dataloader,task4_test_dataloader = dataLoader4(base_model_name,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSEQCLF(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super(BertForSEQCLF, self).__init__()\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        logits = self.classifier(sequence_output[:, 0])  # Take the [CLS] token's hidden state\n",
    "        return logits\n",
    "\n",
    "class BertForTextSummarization(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BertForTextSummarization, self).__init__()\n",
    "        self.decoder = nn.Linear(hidden_size, hidden_size)  # You may want to use a more sophisticated decoder\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        return self.decoder(sequence_output)\n",
    "    \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BertForSTS(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BertForSTS, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, 1)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, pooled_output):\n",
    "        # pooled_output = sequence_output[:, 0]  # Using [CLS] token output\n",
    "        logits = self.dense(pooled_output)\n",
    "        # scaled_logit = 5 * self.sigmoid(logits)\n",
    "         # Approximate sigmoid using two ReLUs\n",
    "        approx_sigmoid = F.relu(logits) - F.relu(logits - 5)\n",
    "        return approx_sigmoid\n",
    "        # return scaled_logit\n",
    "        \n",
    "class BertForQuestionAnswering(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BertForQuestionAnswering, self).__init__()\n",
    "        self.qa_outputs = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        return {'start_logits' :start_logits, \"end_logits\" : end_logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedModel(nn.Module):\n",
    "    def __init__(self, base_model_name):\n",
    "        super(UnifiedModel, self).__init__()\n",
    "        config = BertConfig.from_pretrained(base_model_name)\n",
    "        self.base_model = BertModel.from_pretrained(base_model_name, config=config)\n",
    "        \n",
    "        task1_lora_config = LoraConfig(\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            r=4,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.01,\n",
    "            target_modules=['query', 'value']\n",
    "        )\n",
    "        task2_lora_config = LoraConfig(\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            r=4,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.01,\n",
    "            target_modules=['query', 'value']\n",
    "        )\n",
    "        task3_lora_config = LoraConfig(\n",
    "            task_type=\"SEQ2SEQ_LM\",\n",
    "            r=4,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.01,\n",
    "            target_modules=['query', 'value']\n",
    "        )\n",
    "        \n",
    "        self.base_model.add_adapter(task1_lora_config, adapter_name=\"adapter_task1\")\n",
    "        self.base_model.add_adapter(task2_lora_config, adapter_name=\"adapter_task2\")\n",
    "        self.base_model.add_adapter(task3_lora_config, adapter_name=\"adapter_task3\")\n",
    "        \n",
    "        self.task1_head = BertForSEQCLF(self.base_model.config.hidden_size, 2)\n",
    "        self.task2_head = BertForSTS(self.base_model.config.hidden_size)\n",
    "        self.task3_head = BertForQuestionAnswering(self.base_model.config.hidden_size)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids = None, task=\"task1\"):\n",
    "        \n",
    "        if task == \"task1\":\n",
    "            self.base_model.set_adapter(\"adapter_task1\")\n",
    "            base_outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "            sequence_output = base_outputs[0]\n",
    "            return self.task1_head(sequence_output)\n",
    "        elif task == \"task2\":\n",
    "            self.base_model.set_adapter(\"adapter_task2\")\n",
    "            base_outputs = self.base_model(input_ids, attention_mask=attention_mask,token_type_ids = token_type_ids)\n",
    "            sequence_output = base_outputs.pooler_output\n",
    "            return self.task2_head(sequence_output)\n",
    "        elif task == \"task3\":\n",
    "            self.base_model.set_adapter(\"adapter_task3\")\n",
    "            base_outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "            sequence_output = base_outputs[0]\n",
    "            return self.task3_head(sequence_output)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task: {task}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Corrected train function\n",
    "def train(model, train_loader, val_loader, loss_fn, num_epochs=3, learning_rate=5e-5, task='task1'):\n",
    "    best_model = model\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['answer_start'].to(device)\n",
    "            end_positions = batch['answer_end'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, task=task)\n",
    "            start_logits, end_logits = outputs['start_logits'], outputs['end_logits']\n",
    "            start_logits = start_logits.squeeze(-1)\n",
    "            end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "            start_loss = loss_fn(start_logits, start_positions)\n",
    "            end_loss = loss_fn(end_logits, end_positions)\n",
    "            loss = start_loss + end_loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        cur_loss, _ = validate(model, val_loader, loss_fn, task)\n",
    "        \n",
    "        if cur_loss < best_loss:\n",
    "            best_model = model\n",
    "            best_loss = cur_loss\n",
    "            # Save the best model\n",
    "            torch.save(best_model.state_dict(), os.path.join(save_directory, 'best_model.pt'))\n",
    "            print(f\"New best model saved with validation loss: {cur_loss:.4f}\")\n",
    "            \n",
    "    return best_model\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import evaluate  # Assuming you have the `evaluate` library installed\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def validate(model, val_loader, loss_fn, task='task1'):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_y_start = []\n",
    "    val_y_end = []\n",
    "    val_pred_start = []\n",
    "    val_pred_end = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['answer_start'].to(device)\n",
    "            end_positions = batch['answer_end'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, task=task)\n",
    "            start_logits, end_logits = outputs['start_logits'], outputs['end_logits']\n",
    "            start_logits = start_logits.squeeze(-1)\n",
    "            end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "            start_loss = loss_fn(start_logits, start_positions)\n",
    "            end_loss = loss_fn(end_logits, end_positions)\n",
    "            loss = start_loss + end_loss\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            val_pred_start.extend(start_logits.argmax(dim=-1).cpu().numpy())\n",
    "            val_pred_end.extend(end_logits.argmax(dim=-1).cpu().numpy())\n",
    "            val_y_start.extend(start_positions.argmax(dim=-1).cpu().numpy())\n",
    "            val_y_end.extend(end_positions.argmax(dim=-1).cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    accuracy_start = accuracy.compute(predictions=val_pred_start, references=val_y_start)\n",
    "    accuracy_end = accuracy.compute(predictions=val_pred_end, references=val_y_end)\n",
    "\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f} Start Accuracy: {accuracy_start[\"accuracy\"]:.4f} End Accuracy: {accuracy_end[\"accuracy\"]:.4f}')\n",
    "    \n",
    "    return avg_val_loss, (val_pred_start, val_pred_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_val_test.task3_train_test_val import train as task3_train,validate as task3_validate\n",
    "from architecture import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 12.4112 Start Accuracy: 0.0011 End Accuracy: 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# unified_model = Model(base_model_name)\n",
    "# unified_model.to(device)\n",
    "# z = task3_validate(unified_model,task3_val_dataloader,loss_fn,'task3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5475/5475 [10:47<00:00,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 5.2118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.2308 Start Accuracy: 0.5465 End Accuracy: 0.5876\n",
      "New best model saved with validation loss: 3.2308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5475/5475 [10:48<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.7060 Start Accuracy: 0.6073 End Accuracy: 0.6495\n",
      "New best model saved with validation loss: 2.7060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5475/5475 [10:48<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.8225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.5278 Start Accuracy: 0.6339 End Accuracy: 0.6698\n",
      "New best model saved with validation loss: 2.5278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 5475/5475 [10:48<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.6331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.4158 Start Accuracy: 0.6465 End Accuracy: 0.6847\n",
      "New best model saved with validation loss: 2.4158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 5475/5475 [10:48<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.5227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.3372 Start Accuracy: 0.6513 End Accuracy: 0.6922\n",
      "New best model saved with validation loss: 2.3372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 5475/5475 [10:48<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.4388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2793 Start Accuracy: 0.6550 End Accuracy: 0.6961\n",
      "New best model saved with validation loss: 2.2793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 5475/5475 [10:48<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.3746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2633 Start Accuracy: 0.6631 End Accuracy: 0.7021\n",
      "New best model saved with validation loss: 2.2633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 5475/5475 [10:48<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.3269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2390 Start Accuracy: 0.6631 End Accuracy: 0.7029\n",
      "New best model saved with validation loss: 2.2390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 5475/5475 [10:48<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.2901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2203 Start Accuracy: 0.6654 End Accuracy: 0.7070\n",
      "New best model saved with validation loss: 2.2203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 5475/5475 [10:48<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.2582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1933 Start Accuracy: 0.6692 End Accuracy: 0.7082\n",
      "New best model saved with validation loss: 2.1933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 5475/5475 [10:48<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.2370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1848 Start Accuracy: 0.6677 End Accuracy: 0.7077\n",
      "New best model saved with validation loss: 2.1848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 5475/5475 [10:48<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.2127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1787 Start Accuracy: 0.6684 End Accuracy: 0.7105\n",
      "New best model saved with validation loss: 2.1787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 5475/5475 [10:48<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.1906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1785 Start Accuracy: 0.6713 End Accuracy: 0.7099\n",
      "New best model saved with validation loss: 2.1785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 5475/5475 [10:48<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.1808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1701 Start Accuracy: 0.6710 End Accuracy: 0.7117\n",
      "New best model saved with validation loss: 2.1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 5475/5475 [10:48<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 661/661 [00:40<00:00, 16.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1686 Start Accuracy: 0.6712 End Accuracy: 0.7105\n",
      "New best model saved with validation loss: 2.1686\n"
     ]
    }
   ],
   "source": [
    "# model = task3_train(unified_model, task3_train_dataloader, task3_val_dataloader,loss_fn, num_epochs=15, learning_rate=5e-5,task = 'task3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dataloaders.task1 import dataLoader as dataLoader1\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "task1_train_dataloader,task1_val_dataloader,task1_test_dataloader = dataLoader1(base_model_name,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_val_test.task1_train_test_val import train as task1_train,validate as task1_validate\n",
    "from architecture import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_264926/3327682832.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_dict = torch.load('/home/jyotish/isro/MYProjects/model_checkpoints/best_model.pt')\n",
      "Validation: 100%|██████████| 14/14 [00:03<00:00,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7505 Accuracy : {'accuracy': 0.4908256880733945}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "unified_model = Model(base_model_name)\n",
    "load_dict = torch.load('/home/jyotish/isro/MYProjects/model_checkpoints/best_model.pt')\n",
    "unified_model.load_state_dict(load_dict)\n",
    "unified_model.to(device)\n",
    "\n",
    "z = task1_validate(unified_model,task1_val_dataloader,loss_fn,'task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 1: 100%|██████████| 1053/1053 [08:27<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [00:03<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2521 Accuracy : {'accuracy': 0.9048165137614679}\n",
      "New best model saved with validation loss: 0.2521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1053/1053 [08:27<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [00:03<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2529 Accuracy : {'accuracy': 0.9002293577981652}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1053/1053 [08:28<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [00:03<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2423 Accuracy : {'accuracy': 0.9048165137614679}\n",
      "New best model saved with validation loss: 0.2423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1053/1053 [08:28<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [00:03<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2430 Accuracy : {'accuracy': 0.9105504587155964}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 1053/1053 [08:27<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [00:03<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2412 Accuracy : {'accuracy': 0.9128440366972477}\n",
      "New best model saved with validation loss: 0.2412\n"
     ]
    }
   ],
   "source": [
    "model1 = task1_train(unified_model, task1_train_dataloader, task1_val_dataloader,loss_fn, num_epochs=5, learning_rate=5e-5,task = 'task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 1: 100%|██████████| 1053/1053 [08:27<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [00:03<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2393 Accuracy : {'accuracy': 0.9139908256880734}\n",
      "New best model saved with validation loss: 0.2393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1053/1053 [08:27<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [00:03<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2403 Accuracy : {'accuracy': 0.908256880733945}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1053/1053 [08:28<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [00:03<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2555 Accuracy : {'accuracy': 0.9094036697247706}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1053/1053 [08:27<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [00:03<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2462 Accuracy : {'accuracy': 0.911697247706422}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 1053/1053 [08:28<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [00:03<00:00,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2493 Accuracy : {'accuracy': 0.908256880733945}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = task1_train(model1, task1_train_dataloader, task1_val_dataloader,loss_fn, num_epochs=5, learning_rate=5e-5,task = 'task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 151,301\n",
      "Total parameters: 109,928,453\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "print_trainable_parameters(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_val_test.task2_train_test_val import train as task2_train,validate as task2_validate\n",
    "from architecture import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dataloaders.task2 import dataLoader as dataLoader2\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "task2_train_dataloader,task2_val_dataloader,task2_test_dataloader = dataLoader2(base_model_name,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_266514/1957074267.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_dict = torch.load('/home/jyotish/isro/MYProjects/model_checkpoints/task1e10_best_model.pt')\n",
      "Validation: 100%|██████████| 24/24 [00:05<00:00,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : {'mse': 2.1515507550761046}\n",
      "Validation Loss: 4.6633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss_fn2 = nn.MSELoss()\n",
    "\n",
    "unified_model = Model(base_model_name)\n",
    "load_dict = torch.load('/home/jyotish/isro/MYProjects/model_checkpoints/task1e10_best_model.pt')\n",
    "unified_model.load_state_dict(load_dict)\n",
    "unified_model.to(device)\n",
    "\n",
    "z = task2_validate(unified_model,task2_val_dataloader,loss_fn2,'task2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 1: 100%|██████████| 90/90 [00:43<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:05<00:00,  4.43it/s]\n",
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : {'mse': 0.7376636581532429}\n",
      "Validation Loss: 0.5540\n",
      "New best model saved with validation loss: 0.5540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 90/90 [00:43<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:05<00:00,  4.43it/s]\n",
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : {'mse': 0.7284092391372092}\n",
      "Validation Loss: 0.5402\n",
      "New best model saved with validation loss: 0.5402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 90/90 [00:43<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:05<00:00,  4.44it/s]\n",
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : {'mse': 0.7490308413405967}\n",
      "Validation Loss: 0.5715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 90/90 [00:43<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:05<00:00,  4.44it/s]\n",
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : {'mse': 0.716831657601152}\n",
      "Validation Loss: 0.5235\n",
      "New best model saved with validation loss: 0.5235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 90/90 [00:43<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:05<00:00,  4.44it/s]\n",
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : {'mse': 0.7133034954094484}\n",
      "Validation Loss: 0.5185\n",
      "New best model saved with validation loss: 0.5185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 90/90 [00:43<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:05<00:00,  4.43it/s]\n",
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : {'mse': 0.7137926175937406}\n",
      "Validation Loss: 0.5190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 90/90 [00:43<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:05<00:00,  4.44it/s]\n",
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : {'mse': 0.7262863006212984}\n",
      "Validation Loss: 0.5378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 90/90 [00:43<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:05<00:00,  4.43it/s]\n",
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : {'mse': 0.7198553904376429}\n",
      "Validation Loss: 0.5283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 90/90 [00:43<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:05<00:00,  4.44it/s]\n",
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : {'mse': 0.7222075276532882}\n",
      "Validation Loss: 0.5319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 90/90 [00:43<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:05<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : {'mse': 0.720309877978332}\n",
      "Validation Loss: 0.5290\n",
      "Early stopping at epoch 10 due to no improvement for 5 consecutive epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model3 = task2_train(model3, task2_train_dataloader, task2_val_dataloader,loss_fn2, num_epochs=10, learning_rate=5e-5,task = 'task2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:05<00:00,  4.64it/s]\n",
      "/home/jyotish/isro/MYProjects/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : {'mse': 0.720309877978332}\n",
      "Validation Loss: 0.5290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [00:02<00:00,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2393 Accuracy : {'accuracy': 0.9139908256880734}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "z = task2_validate(model3,task2_val_dataloader,loss_fn2,'task2')\n",
    "z = task1_validate(model3,task1_val_dataloader,loss_fn,'task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_model.to('cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, add_prefix_space=True)\n",
    "\n",
    "val_preds = []\n",
    "val_truth = []\n",
    "print(\"UnTrained model predictions: on validation set\")\n",
    "print(\"--------------------------\")\n",
    "for i,inputs in enumerate(test_dataset):\n",
    "    x = tokenizer(inputs['sentence1'],inputs['sentence2'], truncation=True, padding='max_length', return_tensors='pt')\n",
    "       \n",
    "    logits = model(input_ids=x['input_ids'], attention_mask=x['attention_mask'], token_type_ids = x['token_type_ids'], task='task2')\n",
    "    print(f'Sentence 1 : {inputs['sentence1']} ; Sentence 2 : {inputs['sentence2']} ; TestScore {logits} ; ActualScore {inputs['score']}')\n",
    "    \n",
    "    # for j in range(len(inputs)):\n",
    "    #     print(f'Sentence 1 : {test_dataset['sentence1'][j]} ; Sentence 2 : {test_dataset['sentence2'][j]} ; TestScore {logits[j]} ; ActualScore {test_dataset['score'][j]}')\n",
    "    # break\n",
    "    \n",
    "# print(f'Accuracy on Validation Data : {accuracy.compute(predictions=val_preds, references=val_truth)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1,330,947\n",
      "Total parameters: 111,255,555\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "print_trainable_parameters(unified_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 147,456\n",
      "Total parameters: 124,794,626\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions:\n",
      "--------------------------\n",
      "a feel-good picture in the best sense of the term . - Positive\n",
      "resourceful and ingenious entertainment . - Positive\n",
      "it 's just incredibly dull . - Negative\n",
      "the movie 's biggest offense is its complete and utter lack of tension . - Negative\n",
      "impresses you with its open-endedness and surprises . - Positive\n",
      "unless you are in dire need of a diesel fix , there is no real reason to see it . - Negative\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"--------------------------\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits,1).indices\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions: on validation set\n",
      "--------------------------\n",
      "Accuracy on Validation Data : {'accuracy': 0.9197247706422018}\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "val_preds = []\n",
    "val_truth = []\n",
    "print(\"Trained model predictions: on validation set\")\n",
    "print(\"--------------------------\")\n",
    "for i,text in enumerate(tokenized_dataset['validation']['sentence']):\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits,1).indices\n",
    "    val_preds.append(predictions.tolist()[0])\n",
    "    val_truth.append(tokenized_dataset['validation']['label'][i])\n",
    "    # print(text + \" - \" + id2label[predictions.tolist()[0]])\n",
    "    \n",
    "print(f'Accuracy on Validation Data : {accuracy.compute(predictions=val_preds, references=val_truth)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
